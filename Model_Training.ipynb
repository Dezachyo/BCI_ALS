{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7a019fb8-c924-4d04-b3de-c681a8f5680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easygui import *\n",
    "import pyxdf\n",
    "import PyQt5\n",
    "import mne\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import mne\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from joblib import dump,load\n",
    "import pickle\n",
    "# For interactive plots\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'qt')\n",
    "#from subfunctions import *\n",
    "\n",
    "\n",
    "from mne.decoding import (SlidingEstimator, GeneralizingEstimator, Scaler,\n",
    "                          cross_val_multiscore, LinearModel, get_coef,\n",
    "                          Vectorizer, CSP)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os,numpy as np,pandas as pd\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# MNE functions\n",
    "from mne import Epochs,find_events\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "\n",
    "# Scikit-learn and Pyriemann ML functionalities\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "from pyriemann.estimation import ERPCovariances, XdawnCovariances, Xdawn\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.classification import MDM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyriemann.utils.viz import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aad3e31-8ee9-4f96-b6f0-ad23a7aa9074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BandwidthFilter:\n",
    "    def __init__(self, l_freq, h_freq, method):\n",
    "        self.l_freq = l_freq\n",
    "        self.h_freq = h_freq\n",
    "        self.method = method\n",
    "        \n",
    "class model:\n",
    "    def __init__(self, clsf, input_shape, ch_names,bandwidthfilter):\n",
    "        self.clsf = clsf\n",
    "        self.input_shape = input_shape\n",
    "        self.ch_names = ch_names\n",
    "        self.bandwidthfilter = bandwidthfilter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7278a30-e79a-470c-9712-66d4453fc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file_name = 'Or_1304'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9e5d1bb-1bdd-46e5-bc38-aa2f70ac760c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading c:\\Users\\user\\Documents\\BCI_ALS\\BCI_ALS\\Data\\Processed Data\\Or_1304_Processed ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     496.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "687 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "current_path = pathlib.Path().absolute()  \n",
    "data_fname = current_path /'Data'/'Processed Data'/ (processed_file_name + '_Processed')\n",
    "epochs = mne.read_epochs(data_fname)\n",
    "\n",
    "#read the pickle file\n",
    "picklefile = open(current_path /'Data'/'Processed Data'/ (processed_file_name+'_Filter'), 'rb')\n",
    "#unpickle the dataframe\n",
    "bandwidth_filter = pickle.load(picklefile)\n",
    "#close file\n",
    "picklefile.close()\n",
    "\n",
    "#bandwidth_filter = load(current_path /'Data'/'Processed Data'/ (processed_file_name+'_Filter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4f9b7bcf-359c-4c88-a9b8-49fdb5f91d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = epochs.get_data()  # MEG signals: n_epochs, n_meg_channels, n_times\n",
    "X = epochs.get_data() * 1e6 # mV\n",
    "# Remove baslineperiod\n",
    "baseline_duration = -0.2\n",
    "onset_sample = int(np.absolute(baseline_duration) * 125)\n",
    "X = X[:,:,onset_sample::]\n",
    "\n",
    "y = epochs.events[:, 2]  # target: auditory left vs visual left\n",
    "y = y - (max(y)-1) # 0's and 1's instead of 1's and 2's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "80e2c0d0-b7fa-46dd-873d-fc9630cc7cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_weights = {0:1,1:8}\n",
    "\n",
    "clfs = OrderedDict()\n",
    "clfs['Vect + LR'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(class_weight=class_weights))\n",
    "clfs['Vect + RegLDA'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "clfs['Xdawn + RegLDA'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "\n",
    "clfs['XdawnCov + TS'] = make_pipeline(XdawnCovariances(estimator='oas'), TangentSpace(), LogisticRegression(class_weight=class_weights))\n",
    "clfs['XdawnCov + MDM'] = make_pipeline(XdawnCovariances(estimator='oas'), MDM())\n",
    "\n",
    "\n",
    "#clfs['ERPCov + TS'] = make_pipeline(ERPCovariances(), TangentSpace(), LogisticRegression())\n",
    "#clfs['ERPCov + MDM'] = make_pipeline(ERPCovariances(), MDM())\n",
    "\n",
    "\n",
    "#times = epochs.times\n",
    "\n",
    "\n",
    "# define cross validation\n",
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=42)\n",
    "\n",
    "# run cross validation for each pipeline\n",
    "auc = []\n",
    "methods = []\n",
    "for m in clfs:\n",
    "    res = cross_val_score(clfs[m], X, y==1, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "    auc.extend(res)\n",
    "    methods.extend([m]*len(res))\n",
    "\n",
    "results = pd.DataFrame(data=auc, columns=['AUC'])\n",
    "results['Method'] = methods\n",
    "\n",
    "plt.figure(figsize=[8,4])\n",
    "sns.barplot(data=results, x='AUC', y='Method')\n",
    "plt.xlim(0.2, 0.85)\n",
    "sns.despine()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01b7b1e8",
   "metadata": {},
   "source": [
    "## Plot confusion matrixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6427934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ratio = 0.75\n",
    "test_ratio = 0.25\n",
    "\n",
    "# train is now 50% of the entire data set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=1 - train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c9b3bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vect + LR Classification accuracy: 0.674419 \n",
      "Vect + RegLDA Classification accuracy: 0.738372 \n",
      "Xdawn + RegLDA Classification accuracy: 0.808140 \n",
      "XdawnCov + TS Classification accuracy: 0.529070 \n",
      "XdawnCov + MDM Classification accuracy: 0.494186 \n"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(clfs.__len__())\n",
    "\n",
    "#logisticregression\n",
    "for i,m in enumerate(clfs):\n",
    "    clfs[m].fit(X_train, Y_train)\n",
    "    preds_rg  = clfs[m].predict(X_test)\n",
    "    # Printing the results\n",
    "    acc2         = np.mean(preds_rg == Y_test)\n",
    "    print(f\"{m} Classification accuracy: %f \" % (acc2))\n",
    "    names        = ['non-target', 'target']\n",
    "    #plt.figure()\n",
    "    \n",
    "    #fig,axs = plt.subplots()\n",
    "    ConfusionMatrixDisplay(confusion_matrix(Y_test,preds_rg)).plot(ax= axs[i])\n",
    "    axs[i].set_title(m)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fff989-e3f9-442e-b4bc-e599a1055c34",
   "metadata": {},
   "source": [
    "## Lets fit the model to the data and save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff5087c-183c-4f8b-aaa4-f5681fe3000c",
   "metadata": {},
   "source": [
    "### First, Choose the best model to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d649c447-9fdb-4125-a35a-ff14228b18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = clfs['Vect + LR'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94ebd4d1-0bb6-4ee7-8df3-67fb1ef066c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X,y)\n",
    "input_shape = X.shape\n",
    "ch_names = epochs.ch_names\n",
    "\n",
    "\n",
    "m = model(pipe,input_shape,ch_names,bandwidth_filter)\n",
    "\n",
    "\n",
    "fname = processed_file_name+'_model'\n",
    "path_fname = current_path /'Models'/ fname\n",
    "\n",
    "\n",
    "#create a pickle file\n",
    "picklefile = open(path_fname, 'wb')\n",
    "#pickle the dictionary and write it to file\n",
    "pickle.dump(m, picklefile)\n",
    "#close the file\n",
    "picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9a3d6e0-76f7-4b8c-bca9-808515041fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the pickle file\n",
    "picklefile = open(path_fname, 'rb')\n",
    "#unpickle the dataframe\n",
    "loaded = pickle.load(picklefile)\n",
    "#close file\n",
    "picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e751379-f1e7-4d02-84f8-17222f6dd433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383aa94-ad17-4111-976c-10bee868be60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
